{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/bin/id: cannot find name for user ID 19744\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/cs/student/projects1/dsml/2023/hbosilko/co2_inference/test\n",
      "/usr/bin/id: cannot find name for user ID 19744\n",
      "Thu Aug 15 02:06:11 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.54.03              Driver Version: 535.54.03    CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce GTX TITAN X     On  | 00000000:02:00.0 Off |                  N/A |\n",
      "| 29%   61C    P2              76W / 250W |   9451MiB / 12288MiB |      6%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA GeForce GTX TITAN X     On  | 00000000:03:00.0 Off |                  N/A |\n",
      "| 28%   62C    P2              75W / 250W |   9381MiB / 12288MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA GeForce GTX TITAN X     On  | 00000000:82:00.0 Off |                  N/A |\n",
      "| 24%   57C    P2              73W / 250W |   9381MiB / 12288MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA GeForce GTX TITAN X     On  | 00000000:83:00.0 Off |                  N/A |\n",
      "| 27%   60C    P2              75W / 250W |  11284MiB / 12288MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A     20918      C   python                                     9446MiB |\n",
      "|    1   N/A  N/A     20918      C   python                                     9376MiB |\n",
      "|    2   N/A  N/A     20918      C   python                                     9376MiB |\n",
      "|    3   N/A  N/A     20918      C   python                                     9376MiB |\n",
      "|    3   N/A  N/A     25839      C   ...niconda3/envs/watts0/bin/python3.10     1900MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!pwd\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HF_HOME'] = '/cs/student/projects1/dsml/2023/hbosilko'\n",
    "# os.environ\n",
    "\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cs/student/projects1/dsml/2023/hbosilko/miniconda3/envs/watts0/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "dataset = \"squad_v2\"\n",
    "split = \"validation\"\n",
    "sample_size = 100\n",
    "\n",
    "dset = load_dataset(dataset, split=split, streaming=True).shuffle(seed=seed).take(sample_size)\n",
    "dset = Dataset.from_generator(lambda: dset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task-specific"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_models = ['distilbert/distilbert-base-uncased-distilled-squad',\n",
    "            'distilbert/distilbert-base-cased-distilled-squad',\n",
    "            'deepset/roberta-base-squad2',\n",
    "            'google-bert/bert-large-uncased-whole-word-masking-finetuned-squad',\n",
    "            'timpal0l/mdeberta-v3-base-squad2',\n",
    "            'deepset/tinyroberta-squad2',\n",
    "            'deepset/electra-base-squad2',\n",
    "            'deepset/bert-large-uncased-whole-word-masking-squad2'\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cs/student/projects1/dsml/2023/hbosilko/miniconda3/envs/watts0/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "/cs/student/projects1/dsml/2023/hbosilko/miniconda3/envs/watts0/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "/cs/student/projects1/dsml/2023/hbosilko/miniconda3/envs/watts0/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at google-bert/bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/cs/student/projects1/dsml/2023/hbosilko/miniconda3/envs/watts0/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "/cs/student/projects1/dsml/2023/hbosilko/miniconda3/envs/watts0/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "/cs/student/projects1/dsml/2023/hbosilko/miniconda3/envs/watts0/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "/cs/student/projects1/dsml/2023/hbosilko/miniconda3/envs/watts0/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at deepset/bert-large-uncased-whole-word-masking-squad2 were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/cs/student/projects1/dsml/2023/hbosilko/miniconda3/envs/watts0/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from eco2ai import Tracker\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "torch.cuda.set_device(3)\n",
    "\n",
    "task_answers = {}\n",
    "\n",
    "for m in task_models:\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    qa_pipeline = pipeline(\"question-answering\",\n",
    "                        model=m,\n",
    "                        device=3\n",
    "                        )\n",
    "\n",
    "    for i in range(1):\n",
    "\n",
    "        tracker = Tracker(experiment_description=\"Question answering\",\n",
    "                        project_name=f\"Dataset: {dataset}; Model: {m}; Sample size: {sample_size}\",\n",
    "                        measure_period=1,\n",
    "                        alpha_2_code=\"GB\",\n",
    "                        file_name = \"eco_emissions.csv\",\n",
    "                        ignore_warnings=True\n",
    "                        )\n",
    "        tracker.start()\n",
    "\n",
    "\n",
    "        # count=0\n",
    "        for p in dset:\n",
    "            # print(count)\n",
    "            model_answer = qa_pipeline({'question': p['question'],\n",
    "                                        'context': p['context']}\n",
    "                                        )\n",
    "            # count+=1\n",
    "            if p['id'] not in task_answers.keys():\n",
    "                task_answers[p['id']] = {'title':p['title'],\n",
    "                                         'context':p['context'],\n",
    "                                         'question':p['question'],\n",
    "                                         'answer':p['answers']['text'][0] if p['answers']['text'] else ''\n",
    "                }\n",
    "            task_answers[p['id']][f'{m}_answer'] = model_answer['answer']\n",
    "            task_answers[p['id']][f'{m}_score'] = model_answer['score']\n",
    "\n",
    "        tracker.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(task_answers).T.to_csv('/cs/student/projects1/dsml/2023/hbosilko/co2_inference/test/task_answers.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General-purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_models = [\n",
    "    \"meta-llama/Meta-Llama-3.1-8B\",\n",
    "    \"meta-llama/Meta-Llama-3-8B\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_until_newline(input_string):\n",
    "    # Find the index of the first newline character\n",
    "    newline_index = input_string.find('\\n')\n",
    "    \n",
    "    # If there's no newline character, return the original string\n",
    "    if newline_index == -1:\n",
    "        return input_string\n",
    "    \n",
    "    # Return the substring from the beginning up to (but not including) the first newline\n",
    "    return input_string[:newline_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_llama_inference(dataset, pipe):\n",
    "\n",
    "    count=0\n",
    "    for p in dataset:\n",
    "        print(count)\n",
    "        model_answer = pipe(p['question'] + ' Find the answer to the question in the following text: \"'+p['context']+'\" Answer:', return_full_text=False)[0]['generated_text']\n",
    "        p['model_answer'] = strip_until_newline(model_answer)\n",
    "        answers.append(p)\n",
    "        count+=1\n",
    "\n",
    "    return answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eco2ai import Tracker\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "torch.cuda.set_device(3)\n",
    "\n",
    "gen_answers = {}\n",
    "\n",
    "for m in gen_models:\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    qa_pipeline = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=m,\n",
    "        model_kwargs={\n",
    "            \"torch_dtype\": torch.float16,\n",
    "            \"quantization_config\": {\"load_in_4bit\": True}\n",
    "        },\n",
    "        token='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',\n",
    "        pad_token_id=128001,\n",
    "        max_new_tokens=32,\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "\n",
    "    for i in range(1):\n",
    "\n",
    "        tracker = Tracker(experiment_description=\"Question answering\",\n",
    "                        project_name=f\"Dataset: {dataset}; Model: {m}; Sample size: {sample_size}\",\n",
    "                        measure_period=1,\n",
    "                        alpha_2_code=\"GB\",\n",
    "                        file_name = \"eco_emissions.csv\",\n",
    "                        ignore_warnings=True\n",
    "                        )\n",
    "        tracker.start()\n",
    "\n",
    "\n",
    "        # count=0\n",
    "        for p in dset:\n",
    "            # print(count)\n",
    "            model_answer = qa_pipeline(p['question'] + ' Find the answer to the question in the following text: \"'+p['context']+'\" Answer:', return_full_text=False)\n",
    "        \n",
    "            # count+=1\n",
    "            if p['id'] not in task_answers.keys():\n",
    "                gen_answers[p['id']] = {'title':p['title'],\n",
    "                                        'context':p['context'],\n",
    "                                        'question':p['question'],\n",
    "                                        'answer':p['answers']['text'][0] if p['answers']['text'] else ''\n",
    "                }\n",
    "            gen_answers[p['id']][f'{m}_answer'] = strip_until_newline(model_answer[0]['generated_text'])\n",
    "            # gen_answers[p['id']][f'{m}_score'] = model_answer['score']\n",
    "\n",
    "        tracker.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
